<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Mnemnion]]></title>
  <link href="http://mnemnion.github.io/atom.xml" rel="self"/>
  <link href="http://mnemnion.github.io/"/>
  <updated>2013-08-04T20:05:13-07:00</updated>
  <id>http://mnemnion.github.io/</id>
  <author>
    <name><![CDATA[Sam Atman]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Tangled Web We Weave]]></title>
    <link href="http://mnemnion.github.io/blog/2013/08/03/a-tangled-web-we-weave/"/>
    <updated>2013-08-03T11:13:00-07:00</updated>
    <id>http://mnemnion.github.io/blog/2013/08/03/a-tangled-web-we-weave</id>
    <content type="html"><![CDATA[<p><a href="http://www-cs-faculty.stanford.edu/%7Euno/lp.html">Literate Programming</a> is one of those paradigms whose fate is continual reinvention. I&#39;ve been noticing that my software projects start as Markdown. It stands to reason that they should end up as Markdown as well.</p>

<p><a href="https://help.github.com/articles/github-flavored-markdown">Git Flavored Markdown</a>, in particular, is crying out for a literate, multi-model programming system. The mechanism of named fenced code blocks lets one put multiple languages in a single file, and they will already be syntax highlighted according to the named language. </p>

<p>As literate programming is for the ages, we shall call our system <a href="README.md">Marmion</a>. The weaver shall be known as <a href="athena.md">Athena</a>; the tangler, <a href="">Arachne</a>.</p>

<p>If at all possible, we don&#39;t want to touch GFM itself. Therefore, here are some principles:</p>

<ul>
<li><p>Code in fenced code blocks is extracted, macro-expanded, and executed in whatever ways are appropriate.</p></li>
<li><p>Macros must employ patterns not used in a given language; therefore, we must be able to define those patterns.</p></li>
<li><p>All configuration happens in special code blocks, called <code>```config</code>:</p></li>
</ul>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">{</span> <span class="ss">:name</span> <span class="s">&quot;A config file&quot;</span>,
</span><span class='line'>  <span class="ss">:format</span> <span class="ss">:edn</span>
</span><span class='line'>  <span class="ss">:magic-number</span> <span class="mi">42</span> <span class="p">}</span> <span class="c1">;this is actually tagged ```clojure</span>
</span></code></pre></td></tr></table></div></figure>

<ul>
<li><p>Code in regular unfenced code blocks is not included in the weave. Nor are fenced code blocks that aren&#39;t reached from the top macro. The code above, for example, <em>will not</em> be in the finished weave, because it is exemplary.</p></li>
<li><p>All text ends up in the tangle, which is an HTML file. No other tangle format is contemplated. </p></li>
<li><p>If standardized, the tangle format will not be specified, only the markup format and the requirements for the subsequent weave. HTML is a moving target, as is visual display in general. </p></li>
<li><p>The Markdown may be extended, but only in the same way as any other code: by specifying a macro template and expanding it from provided code. It is the macro-expanded Markdown which is tangled and woven.</p></li>
<li><p>Corollary: the Markdown is macro expanded before anything in a code block.  </p></li>
<li><p>Corollary: the Markdown macro will be standard. There should be no reason to include it. Because Clojure is the implementation language, and has a defined reader macro syntax, this is already true of Clojure(Script).</p></li>
<li><p>The weaver should visit all internal links in search of code. Some tag in HTML form should be provided so that fully-marked-up links, so tagged, will also be followed in search of exterior code. </p></li>
<li><p>If exterior code is requested, it is added to the source as a fenced code block. The tangle will preserve the link directly above the code block. Some sensible effort will be made to infer the code format from the file extension. This is to be done before macro expansion, so that if there are macros in the exterior code, they will be expanded.</p></li>
<li><p>We should maintain a set of canonical macro patterns for languages, to encourage mutual compatibility in source and tangled code.</p></li>
<li><p>No mechanism for transclusion on the file level will be provided. The file structure of the Markdown is the file structure of the tangle. Working around this using the tagged-link method will leave a broken link in your tangle.</p></li>
</ul>

<p>This is the sort of project that we can tackle in stages. The most important part is the weaver, because we have a fine tangler in the form of <a href="http://jekyllrb.com/">Jekyll</a>. </p>

<p>This is a job for <a href="http://clojure.org">Clojure</a>. The weaver and perhaps the tangler will be Clojurescript compatible in the narrow sense, but useless unless Instaparse is ported, which seems unlikely, though you never know. </p>

<p>Clojure is chosen for a few reasons. <a href="https://github.com/edn-format/edn">EDN</a>, for one, which will be the format of any <code>```config</code> code block. Also because of <a href="https://github.com/Engelberg/instaparse">Instaparse</a>, for which the usual regular-expression based markup approach is a strict subset of capabilities. It has the best story I&#39;m aware of for setting regular expressions declaratively in a data format, which is exactly how we will provide macros. </p>

<p>To be clear, this will let us syntax highlight a provided macro in a distinctive way, and put things like the colors to use right in the markdown. This is only useful with a completed weaver; Pygments will get the macros wrong but this is a minor stylistic matter which can be corrected by retangling with a better highlighter. </p>

<p>Instaparse is my go-to choice for writing flexible parsers that are meant to be shared, so Clojure it is. I hope Instaparse catches on to the point where it becomes core, and hence worth maintaining separate <code>.clj</code> and <code>.cljs</code> versions. </p>

<p>The first, and most important step, is writing <a href="athena.md">Athena</a>, the weaver. The weaver does the following: finds all the <code>```config</code> code, parses it to configure itself, then goes after the code blocks, and uses the macros and config information to construct the weave. Finally, it calls the trigger file, which must contain everything needed to build the weave into an executable, or whatever the final product is.</p>

<p>The tangler, <a href="">Arachne</a>, should be a <a href="https://github.com/mnemnion/jekyll">fork of Jekyll</a>, with a low surface area of interaction. What I mean by this is that merges between the bases should avoid touching one another&#39;s files wherever possible. The only changes I contemplate personally is to plug-replace the syntax highlighter, for several reasons. </p>

<p>Pygments requires one to write actual code to markup a new format. This is distasteful. Also, we need to markup the macros, which we won&#39;t know until we weave the code. Furthermore, a static syntax highlighter should be based on a powerful parser, not a regular engine janked up with extra Python. For pity&#39;s sake. </p>

<p>If Marmion becomes popular, someone might want to write advanced capabilities: putting compatible code in a REPL, for example, or linking to one from the code, or linking to the line number in a public Github repository generated by the weaver. The last is particularly powerful. All of this will assuredly be easier with a parser-backed tangler. </p>

<p>This is the only way I have to tackle large problems: recursing through the Big Project until I hit something atomic and critical to further progress. Arc leads to GGG, which will benefit greatly from a literate style, which leads to Marmion. Marmion built, writing GGG in an understandable way becomes possible. </p>

<p>I think I&#39;ve painted myself into a corner, as I can&#39;t think of anything offhand which I need to write in order to write Marmion. </p>

<p>Time to generate more Markdown!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Syntax for Literal Strings]]></title>
    <link href="http://mnemnion.github.io/blog/2013/07/27/syntax-for-literal-strings/"/>
    <updated>2013-07-27T10:09:00-07:00</updated>
    <id>http://mnemnion.github.io/blog/2013/07/27/syntax-for-literal-strings</id>
    <content type="html"><![CDATA[<p>I find it somewhat astonishing that the languages with which I&#39;m familiar still start and end strings with the same character. It is as though we used <code>|</code> for <code>{</code> and <code>}</code> in code blocks and relied on context to figure out if it was <code>begin</code> or <code>end</code>. </p>

<p>Incidentally, it&#39;s quite possible to write a language this way, and an interesting exercise. <code>for | i = 0 ; i &lt; 2 ; i++ || codeBlock |</code> should parse just fine. Heaven help you if you misplace anything. </p>

<p>Check out <a href="https://en.wikipedia.org/wiki/Delimiter#Bracket_delimiters">bracket delimiters</a> on the Wiki. Two of these things are not like the others. Those two are used preponderantly for strings. </p>

<p>It&#39;s clear enough how it happened. A string has an obvious mapping to literary quotation: <code>&quot;That&#39;s what she said!&quot;</code>.  ASCII gives us exactly three options: <code>&#39;</code>, <code>`</code>, and <code>&quot;</code>. <a href="http://c-programming.itags.org/q_c-programming-language_16297.html">It turns out</a> that C was defined using 91 characters, and <code>`</code> was not among them. </p>

<p>Meta enough, I&#39;m writing this in Markdown, and to type <code>`</code>, I must type <code>`` ` ``</code>. I will leave how I typed <code>`` ` ``</code> as an exercise for the reader. </p>

<p>So C chose <code>&quot;</code> for string syntax, and <code>&#39;</code> for characters, and these decisions made sense, somewhere in the mists of time. C also initiated the proud tradition of string escaping, which wasn&#39;t invented to get around the delimiter problem, but which can be used for that purpose in a hacky way. String escaping is so you can say <code>\n</code> and get a newline, the incidental benefit is you can say <code>\&quot;</code> and get a <code>&quot;</code>, hence one may include any character in such a string. Two backslashes is of course <code>\\\\</code>. One gets used to it. </p>

<p>Oh hey, just for fun, why not write a regex that will match such strings? Won&#39;t take you long, I promise. I&#39;ll be right here!</p>

<p>To the point. In typography, we don&#39;t do this. We start quotations with <code>‚Äú</code> or <code>‚Äü</code> and end them with <code>‚Äù</code>. On the <a href="http://en.wikipedia.org/wiki/%C2%AB">Continent</a>, <code>¬´</code> and <code>¬ª</code> are used, and this would be my preference as they are much easier to tell apart and don&#39;t have two choices for the opening delimiter. If you follow the link, It turns out they are used both <code>¬´this way¬ª</code> and <code>¬ªthis way¬´</code> and even <code>¬ªthis way¬ª</code> by Finns (<a href="http://en.wikipedia.org/wiki/Finnish_language">of course</a>). We favor the first, because all other brackets in computer programming are inward facing <code>&lt;{[(like so)]}&gt;</code>.</p>

<p>What&#39;s the point? They aren&#39;t on standard keyboards in the US; while any worthwhile editor can get around this, there&#39;s a pain point there. Some people will argue a virtue in using ASCII for source code, and while those people <a href="https://github.com/cgyarvin/urbit">have a point</a>, the ship sailed a long time ago. We use Unicode, and it isn&#39;t going anywhere. </p>

<p>The point is that, without proper left-right matched strings, you cannot literally quote your own source code within your source code. This is damaged, for any language that lets you evaluate at runtime (the interesting ones IOW). If we use <code>¬´</code> and <code>¬ª</code>, we can use bog-standard reference counting to assure that any properly-balanced literal strings in the source code get quoted. Since in this imaginary syntax a bare <code>¬ª</code> not balanced on the left with a <code>¬´</code> is a syntax error, any correct program can be embedded. </p>

<p>If, for any reason, you need a bare <code>¬ª</code>, why not use the ISO standard SHA-1 hash of the Unicode value of <code>¬ª</code>? Why not indeed. It then becomes impossible to literally quote that one hash, which is officially the point where it is perverse to pursue the matter further. Concatenate for that one. </p>

<p>To be clear, <code>&quot;</code> for escaped strings is concise and well understood, and with enough convolutions one may write as one pleases. It&#39;s syntax such as <code>&#39;&#39;&#39;</code> for literal strings that grates against my sensibilities. </p>

<p>Clojure has no syntax for literal, multi-line, unescaped strings. That&#39;s too bad; no one does syntax like Rich Hickey, and I suspect that the inadequacy of existing options plays a role here. He may not be willing to go off-keyboard, but I feel that the <code>¬´</code> and <code>¬ª</code> syntax has a lot to offer. Certainly Europeans would be pleased. </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction To The Architecture]]></title>
    <link href="http://mnemnion.github.io/blog/2013/07/27/introduction-to-the-architecture/"/>
    <updated>2013-07-27T00:00:00-07:00</updated>
    <id>http://mnemnion.github.io/blog/2013/07/27/introduction-to-the-architecture</id>
    <content type="html"><![CDATA[<p>Hello, World!&#8212;<br>
layout: post<br>
title: &quot;Introduction to the Architecture&quot;<br>
date: 2013-07-27 12:20<br>
comments: true<br>
published: false</p>

<h2 id="toc_7">categories: Code, Arc</h2>

<h2 id="toc_8">Notes on a Sensible Environment for Computation.</h2>

<p>The goal is, superficially, a simple one. I wish to be able to turn on my computer, and reset it to one year ago. I can then move around inside that space and do new things, like fetch information from one year ago, rerun programs exactly as they ran then, and the like. </p>

<p>That&#39;s not the whole goal. That&#39;s a taste of the goal. It suffices to start the conversation.</p>

<p>My name for this project is the Arc. <a href="http://www.paulgraham.com/arc.html">Paul Graham</a> will just have to get used to that; the concept is not his, and the word is too ancient and noble to languish as an experimental dalliance. To differentiate, we may call it Araka, and I will use the two interchangeably. There is no difference, any more than LISP and Lisp are two canonically different things. Vowels simply aren&#39;t that important unless they begin a word. Nor does the distinction between c and k matter, when they are sounded the same. </p>

<p>This is another taste of the goal. The Arc is Architecture, not Environment. </p>

<p>Hi. I&#39;m your host, mnemnion. My day job is to make heterogenous computing environments of multi-core CPU and GPU architectures do computation as fast as possible. When I say <a href="http://en.wikipedia.org/wiki/Von_Neumann_architecture">John von Neumann</a> is fired, I mean it.</p>

<p>For another taste of the goal, I present <a href="http://www.loper-os.org/?p=284">LoperOS</a>. Loper would appear to be hard at work on his own version of the goal. I eagerly await. </p>

<p>The Seven Laws of Sane Personal Computing are <a href="http://en.wikipedia.org/wiki/Three_Laws_of_Robotics">Asimovian</a> in scope and breadth. Certain clauses, and the soul of Rule IV, are sociopolitical in nature: Show me a readable program and a compiler for it, and I will show you complete gibberish that does the same thing. This should not keep us from doing our level best, and is no excuse at all for failing at the purely technical points. </p>

<p>Another player in the game is <a href="https://github.com/cgyarvin/urbit">C. Guy Yarvin</a>, whose Urbit is a credible attempt at the whole shebang that you can actually go and marvel at right now. It&#39;s written in Martian and there is no dictionary, the Rosetta is in C and hard going, and <a href="http://moronlab.blogspot.com/">no, I am not exaggerating</a>. The goal appears to be to write the damn thing rather than, I dunno, blogging about it. Laudable; I eagerly await. </p>

<p>What with the aforementioned day job, when I write code on my own time, <a href="https://github.com/mnemnion/emojure">I go light</a>. Whimsical, even. What I&#39;m going for with these posts is an exploration of the <a href="http://en.wikipedia.org/wiki/The_Nature_of_Order">patterns</a> that will eventually make up the Architecture. </p>

<p>An aside: if you haven&#39;t read <em>The Nature of Order</em> you&#39;ve missed what Alexander has to offer. <em>A Pattern Language</em>, while a brilliant gem, is not the Magnum Opus. </p>

<h2 id="toc_9">Arc and Arcism</h2>

<p>The Arc is part of a larger project, the arcist tendency. The Free Software Movement, which is the only technocratic political movement of any significance thus far, is thoroughly rooted in International Socialism. I say that with all affection, and hope RMS would agree, though he may prefer to say &quot;progressive&quot;. I&#39;m all about <a href="http://en.wikipedia.org/wiki/Emic_and_etic">emic</a> vocabulary; progressive it is. </p>

<p>The Arc is in this same sense a vehicle for arcism. There is no sense in which the Arc can succeed on its own, as will become clear enough as we explore. As arcism does not rely on controlling the State (though flourishing under such conditions), a modest success at forming Arcist society will suffice for modest success in the Architecture. Modest success is sufficient to the task at hand. </p>

<p>As it happens, I&#39;m making both the Arc and arcism up as I go along. I&#39;ll do what I can to make them separate posts. </p>

<p>To illustrate the difference of approach, we may consider licenses. An arcist considers contracts which cannot be vigorously enforced to be suspect, akin to a pact with a demon, and a weak-willed one at that. The GPL, with its sticky viral influence and lax enforcement in practice, is a source of great amusement.</p>

<p>The arcist recognizes several states code can be in: unavailable, for example, or encrypted with a key that is believed secure at present, or in the public domain. We favor the public domain for anything interesting: it is the nature of software to be most broadly useful if the source code is readily available. </p>

<p>We may also suggest an old-fashioned way to get paid for all this open-source code: guild up and shake the rest of society down for our share of the loot. We are open to other suggestions that are sensible and reality-respecting. </p>

<p>There are&#8230; very few servants of the Emperor at present. The Arc is made of purest vapor. Let us proceed to condense, and see what may trickle into our flask.</p>

<h2 id="toc_10">Rationale</h2>

<p>Two questions worth asking: Why do this Great Work of reimplementing All the Things? Granted that it&#39;s worth doing, what am I bringing to the table? </p>

<p>I stole Arc because naming things is the Second Hard Problem, and arc- solves it. The Arc runs on the Arcitecture, a physical platform that runs ArcOS. Information travels across the Network (yep, same Network) and is cooled and stored via the Arcive protocol. The system language is Araka, or Arc if you prefer. </p>

<p>This intensive focus on the sensory is characteristic of the entire project. Arc, as pronounced by Americans, is a hard word for much of the rest of the world. <code>a(r|l)…ôk…ô</code> can be broadly realized by essentially everyone. </p>

<p>I am a representation nerd. Ask me about the connection between astrology and abstract algebra sometime. No, really, ask me this as often as possible, because I&#39;m not good enough at explaining it yet to write it down. </p>

<p>This drive led me to develop Phon, a writing system, featural in nature, with the same scope as the International Phonetic Alphabet. Phon deserves its own separate series of blog posts, which really should be published before this one. You can write it in all four canonical directions with minimal confusion. That took some doing; the manual runs 180 pages at present, in trade paper format. It should take maybe two weeks to teach a literate ten year old to write her own language, in her preferred direction. </p>

<p>Phon lies at the intersection of linguistics, representation theory, abstract algebra, Hermetics, and Tolkien studies. The astute reader may notice that I have a knack for naming things. Cache validation is less my style, and I have been known to be off by one, from time to time. A youth misspent with Pascal will do that to ya. </p>

<p>One may at least say that I&#39;m coming at the project from a distinctive angle. I believe that reaching the goal begins with representation, and am confident in saying that I&#39;m the only person on the planet, right now, who has read the books I consider critical.<sup><a href="">[citation needed]</a></sup></p>

<p>Meanwhile, I haven&#39;t read all of <em>your</em> critical books, dear Reader. Suggestions are welcome. </p>

<h2 id="toc_11">Representation</h2>

<p>It&#39;s all zeros and ones, kids. Zeros and ones. Everything else is metaphor.</p>

<p>Our metaphor is dominated by typography. You could cast the entirety of Unicode in lead, bring it back 300 years, and induce quivering orgasm in the printers of the day. It would be quite the heavy box. </p>

<p>Code is textual for a reason, tamper with that at your peril. But serious face, folks: with 64 bits of Unicode, largely barren, what earthly sense does it make to represent <strong>executable code</strong> with the same characters you kiss your mother with?</p>

<p>Nope, we keep reusing those bottom bytes like a bunch of monkeys. UTF-16 is still out there, Emoji is a mess of shims and image embeds, and this is not the worst of it. The code points mean only one thing: a name, and a shape that hopefully can be recognized as the same across fonts that goes with the name. üí©, PILE OF POO, Unicode: U+1F4A9 (U+D83D U+DCA9), UTF-8: F0 9F 92 A9.</p>

<p>Do you see the Poo, dear Reader? Try Firefox.</p>

<p>We can only burn the fields and start anew. Somewhere in a dusty corner of our Representation Format, we shall find the Unicodes, along with the all-important canonical translation matrices which allow archaeologists to sensibly render the fragments of the past and make them actually useful again. </p>

<p>MIME is a bit closer to the point, while missing the target entirely. MIME is, from our perspective, just a bunch of bullshit tacked on to the beginning of a file that may or may not help you figure out what the fuck to do with it. That it works well enough in practice is a miracle of discipline. </p>

<p>A useful header would say &quot;the following content is of type Foo. Its identity may be hashed as <code>Q222a27db79ac39dd6ba2fc1901d6b69c</code>, and the content may be validated for type using a function whose hash is <code>222a27db79ac39dd6ba2fc1901d6b69c</code>&quot;. </p>

<p>Entropy being what it is, white noise can be encoded into any data-only format. We cannot expect to assure that an mp3 contains music, nor that a Unicode string can be meaningfully displayed at all, there being many code points that are set aside for private use. </p>

<p>We can validate that Javascript is Javascript, however, by loading it into an interpreter without errors. How much confidence this gains you varies by language. </p>

<p>This is going backwards, however. JSON can be validated for type and could be usefully transmitted with a header as described. It would be a modest improvement. It&#39;s still Unicode, which means any content it provides is buried in some implementation somewhere and subject to rot. The fact that a human could read it and partially decode it is cold comfort. </p>

<p>Part of the goal is to allow archaeologists a fighting chance at decoding 500 year old partially degraded thumb drives. We have, maybe, one shot at this, before Unicode takes on the strength of DNA. </p>

<h2 id="toc_12">Why we care</h2>

<p>There are only two formats, text and binary. Everything else is tacked on. Here&#39;s a number: 42. It&#39;s represented as Unicode, and if I represented it as, say, an integer, it couldn&#39;t coexist in this Markdown file. I would have to put it somewhere else or do something hacky like Base64 encode it. </p>

<p>If it&#39;s binary, it&#39;s either implementing some spec or it&#39;s homebrew. MIME may help you figure out the former, or it may not. There is nothing even vaguely resembling consistency or regularity anywhere in binary land, and the relationship between the textual world and the blobosphere is uneasy. </p>

<p>Where text is concerned, ASCII at least had the virtue of being somewhat narrowly defined in terms of (mostly) glyphs with distinct shapes that most humans can tell apart. Unicode offers no such promise. </p>

<p>At some point, we&#39;re going to be hashing everything and chunking it out onto the network. Data has to represent the same way, in part and in aggregate, for this to work. The only possible shape is a sort tree, so that&#39;s what we&#39;ll end up using. </p>

<h2 id="toc_13">Section</h2>

<p>What we need is a single format that can resolve all conceivable data and code. That&#39;s not as whacky as it sounds: we have hundreds of equally bad formats for doing this already. Most of them are ASCII or Unicode, of which my opinion is clear. This is computation we&#39;re talking about; it&#39;s all zeros and ones, kids. </p>

<p>Here are some of the patterns which constrain this format:</p>

<ul>
<li><p>Data types are defined in a single fashion, that doesn&#39;t differentiate in principle between standard and extended types. </p></li>
<li><p>Data is composable, a single logical file may contain an arbitrary combination of types, and the operating environment can be relied upon to do sensible things with those compositions. </p></li>
<li><p>New types are, in general, old types with new constraints on their composition. </p></li>
<li><p>All data is sortable, such that if the order is arbitrary for a grouping of data, it is encoded the same way each time. This is critical for deduplication of content in the Arcive. </p></li>
</ul>

<p>To contrast this approach with the state of the art: a website contains a logical structure in the form of files and directories, a server that can present a second, logically distinct superstructure of the same format through URLs, and finally pages, presented to the user, which fetch down this content in yet another logical order provided by the formatting of the HTML et al. </p>

<p>In Arc, we present a hash to the network, and chunks of the content arrive until we have enough to display it. The data structure contains text, images, logic, and whatever else we want it to have, including placeholders with hashes that can fetch down yet more content, video shall we say.</p>

<p>We would like to not have to include an entire video in order for the value of the video to be a part of the identity of the container (say the page it&#39;s embedded in). We also don&#39;t want a page to have a different identity depending on whether the content was fetched or physically included. Clearly, our metaformat is nothing but a concatenation of hashes which is itself hashed to provide the identity. </p>

<p>This means once you create a lolcat, it is one and the same lolcat from the perspective of every person who views it and every context in which it is embedded. They are all served from the same hash; as many extra copies are kept around on the Arcive as are convenient, and no more. The minimum is three.  </p>

<p>Also, if you copy and paste the Gettysburg Address from a website, you should end up with the same Gettysburg Address, not a different one. This is transclusion and we want to have it. We can have it, if we give up on the idea that information should have a physical canonical source on the network from which we retrieve it. </p>

<p>Right now, transclusion is called hotlinking, and is bad form. Jesus wept. It seems like a small matter when it&#39;s lolcats, but when it&#39;s multi-GiB scientific data sets, it makes a difference. Right now, a URL citation says &quot;well, we found something here once that was what we&#39;d like you to look at. Good luck!&quot;. </p>

<p>What is needed is a name for data, not an address. A citation should say &quot;this is the identity of the data in question. If you find something with this identity, it is that to which we refer&quot;. An address, if provided, simply says &quot;the computer at this location can probably find the data in question&quot;.</p>

<h2 id="toc_14">Representation and Resilience</h2>

<p>Choosing the wrong level of representation can have severe consequences on resilience. Sound encoding is a good example. A sensible format for sound would treat encoding as entirely separate from what a track <em>is</em>. Not only that, it wouldn&#39;t rely on promises, such as a header that says &quot;this file definitely isn&#39;t Never Gonna Give You up&quot;. It would say something like &quot;regardless of encoding, you&#39;ll find the following sonic fingerprint if you analyze the track with this piece of logic&quot;. The logic would do what our ears do, basically. Two files containing the same song in different encodings would have the same sonic fingerprint header.</p>

<p>This doesn&#39;t have to be pass / fail, either, that&#39;s merely the simplest implementation. For bitmapped data, a mild Photoshopping could still provide say 90% confidence that the file is &#39;the same image&#39;. For anything with a canonical sensory form, this kind of validation works, and there are plenty of proprietary solutions built on this premise: Google Reverse Image Search, Shazaam, and Soundhound, to name three. </p>

<p>Google also does a good job of detecting what natural language a passage is written in. It&#39;s the same trick. The equivalent test for a passage of text would involve OCR on the bitmap that the environment generates to display the text. </p>

<p>The technique is surprisingly general and works for any static representation of data which is ultimately presented to the senses. It cannot tell you if two games written in Python and Clojure are the same game, any more than it could tell you if a movie and a book have the same plot. It could tell you if an audio book and a text file have the same text, however, or take a good guess at it. </p>

<p>While hashes provide a guarantee of exactitude, this kind of fingerprinting can provide a guide to similarity. </p>

<p>There is another kind of exactitude beloved of computers, that of type. Hashing is for identity, for type, we need some other approach. </p>

<h2 id="toc_15">More Unicode Bashing</h2>

<p>There are few topics more muddled in our field than what a type &#39;is&#39;. I mean something relatively simple: that there exists a function which can take a certain input and validate that it may be treated as of a certain type. For instance, we may define a degenerate type, Blob, the validation function for which simply returns true.</p>

<p>This is also the validation function for ASCII, and hence in principle for Unicode. In practice, Unicode is such a beast that one can often reject arbitrary data as unlikely to contain it, given the number of barren code points. </p>

<p>Anyone from the DOS era remembers loading binaries into a text editor, and the resulting beeping and screen vomit. It is possible to interpret any data at all as ASCII, though this is unwise. Off on <a href="http://moronlabs.com">mars</a>, cgy is embracing this peculiarly arithmetical relationship between ASCII and the low numbers to do voodoo. I see the temptation: it&#39;s here, it&#39;s queer, and we&#39;re used to it. </p>

<p>My beef with Unicode begins with its basic paucity. The logic is that of English and Latin writing, falling apart as soon as one reaches French (e, √© etc. have a dictionary order which is not preserved) and becoming wholly ruined somewhere beyond the European mind. Ge&#39;ez, for an example, has a relationship between the characters that isn&#39;t preserved by writing them all out, while the Hanzi are made of radicals, damnitall, they are in no sense some named flat collection of code points and it&#39;s a crying shame to treat them that way. Check out a Chinese input form for how the logic of Hanzi should be handled. There are options. Better options!</p>

<p>It gets ridiculous, and dangerous, with a practice that is universal and placidly accepted: users are encouraged to input text using the same encoding as the interpreted language running the logic governing the local environment. The mind boggles. There has to be a better way. </p>

<p>In my own little backwater, I am also peeved that Unicode&#39;s &quot;one code point per glyph&quot; policy ends up overloading most of the glyphs used by the IPA to represent sound. If I type e, you have no idea what I meant, and Unicode can&#39;t help you. It sucks the root: I can use visual tricks to inform you, the reader, of my intention, but the computer gets left out. </p>

<p>I could go on. I suspect I shall. For now, suffice to say that it is to our advantage to design types which can be validated, and if we can do so rapidly, so much the better. </p>

<h2 id="toc_16">Functional Validation.</h2>

<p>The idea couldn&#39;t be simpler. You have a chunk of data and some reason to believe it might be, say, music. You run it through a function, which may examine it but not alter it, and the function does things. It returns a verdict, true or false. </p>

<p>The function can and should be anything. It is to our advantage if the heavy lifting is done by a grammar. Grammars can be ambiguous, which is a thorn for interpretation but can be a great boon for validation. They are also declarative, serving to isolate at least some of the semantics of execution from the constraints expected.</p>

<p>This is about more than running differently on different architectures. A grammar may be used exactly, in a linear fashion, or it may be used statistically, to provide a degree of confidence that a file is more-or-less of a certain type. This is useful when files are damaged, missing, or when we must make an initial guess as to type because we have no clues. </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Homoiconicity and Data Forms]]></title>
    <link href="http://mnemnion.github.io/blog/2013/06/17/homoiconicity-and-data-forms/"/>
    <updated>2013-06-17T14:52:00-07:00</updated>
    <id>http://mnemnion.github.io/blog/2013/06/17/homoiconicity-and-data-forms</id>
    <content type="html"><![CDATA[<h3 id="toc_6">Representation of Data in Structured Programs.</h3>

<p>Today we&#39;re going to discover a programming language. We&#39;re going to start by contemplating the idea of code as data. </p>

<p>LISP, and by the all-caps I mean the original flavours, had two fundamental forms: atoms, and lists. As Lisp grew up, the lists became able to represent any sort of data type, but at the expense of a certain homoiconicity. </p>

<p>That&#39;s a controversial assertion, but hear me out. A list in a Lisp is a bunch of cons cells, it&#39;s a specific data structure used by default to do pretty much anything. Since the first position (first second third) has a function or a macro, you can fake, say a hash, by saying something like (hash a a-prime b b-prime) but here&#39;s the problem: that&#39;s not homoiconic to your data anymore. Not in a way that accords with modern expectations. </p>

<p>Let&#39;s talk about JSON. Now, JSON is homoiconic to your data. <code>{}</code>? Object. <code>[]</code>? List. <code>&quot;&quot;</code>? String. <code>(1-9)(digits*)</code>? Number. And so on. </p>

<!-- more -->

<p>What makes this homoiconic, and Lisp less so? Strictly, it&#39;s that by the time you reach the data, you know what type of data it is. Before you get to the value of a string, you see the <code>&quot;</code>, before you get to an object, you see <code>{</code>, and so on. In paren-only Lisps, you see <code>(</code>, think &quot;list&quot;, then see a &quot;function&quot;, discover that it&#39;s actually &quot;hash&quot;, and reparse the whole thing as a new data type. This has a cost that adds up over time. Also, the data type is closed exactly like a list, which it isn&#39;t, so finding the close character in a sea of  parentheses is genuinely hard &#8211; though in Lisp, this matters less in practice than one might think.</p>

<p>CL heads will staunchly and indignantly deny all this, and they&#39;re probably right for them: there are reader macros, paren bashing is a totally valid way to close structured data, and so on. But it&#39;s just not how we&#39;d do it now. So let&#39;s start with this JSON business and think through how we&#39;d make a language from it.  </p>

<p>JSON is extracted from JavaScript, of course, so we could just add JavaScript back and call it a day. That&#39;s not the point of this exercise, the point of this exercise is to make a JSONian language from JSON. JSON is JavaScripty, but JavaScript is not JSONian. </p>

<p>We&#39;ll add bare words back first. JSON supports only quoted strings, because JS uses bare symbols for all variables including functions, and JSON isn&#39;t supposed to be able to pass you executable code. But JSONian is all about executable code, so let&#39;s start by putting them back.</p>

<p>But put them where? Inside curly braces? Right now, <code>{ &quot;foo&quot; : &quot;bar&quot; }</code>is what we&#39;re doing with curly braces. In JS that&#39;s a special form, the more normal use of curlies is <code>{ statement; statement; statement; }</code>. Do we want to allow that? </p>

<p>From the JSONian perspective, this would be confusing syntactic overloading. Also, parentheses are not used yet. So we&#39;ll do the Lispy thing, and use them for <code>(function arg arg arg)</code>. It&#39;s simple, and that&#39;s a virtue in a data representation format. Also, unlike Lisp, it will be much less overloaded, because we have <code>[list, list, list]</code> for lists. But let&#39;s call them vectors now, since we now have Lispy lists and don&#39;t want to get confused. </p>

<p>Let&#39;s also wave a magic wand and get rid of some strictures we don&#39;t need. No commas between list elements, no colon between key : value pairs. They aren&#39;t strictly necessary, as list elements are already separated by whitespace and objects won&#39;t compile if they have an odd number of elements. Let&#39;s be nice and let you add commas wherever you want, if it helps you keep track of something. Our language will ignore them.</p>

<p>Furthermore, let&#39;s get rid of the silly strictures on numbers and just say that, to a first approximation, they behave like actual mathematical numbers. It might be nice to add imaginary/complex, but let&#39;s stay grounded: integers, rationals and reals. </p>

<p>Are we done? We could be; this would be a fine language. But let&#39;s refine. One place we can improve: symbols need to resolve to something else or it&#39;s an error, while strings are opaque to the language, that is, the contents of a string is not meaningful to the compiler itself and we don&#39;t want to change that. A compiler won&#39;t know that &quot;foo&quot; isn&#39;t &quot;bar&quot; unless you compare them explicitly. Since we have a convenient colon left over from trimming the fat off our objects (let&#39;s just call them maps, since they are), we can define <code>:foo</code> and <code>:bar</code> as keywords, which always equal themselves. They&#39;re useful in our maps: <code>{:a a :b b}</code> could let us do something like <code>(:a {:a a :b b})</code>, where our keyword acts like a function and retrieves the <code>:a</code> value from the map. <code>(&quot;a&quot; {&quot;a&quot; a &quot;b&quot; b})</code> should be an error, because it makes no sense to &quot;literal string&quot; something. However, there&#39;s no particular reason to restrict the key or value types of our maps, merely the use of this particular syntax sugar. </p>

<p>I could keep being coy, but that&#39;s not the point: this is Clojure, and this is why I think it&#39;s phenomenal and am convinced that Clojure is the branch from which all future Lisps of importance and duration will grow. </p>

<p>Being homoiconic to your primary data types is important, I dare say crucial. For LISP, that was atoms and lists, and for its descendants that are not Clojure, this fundamental duality is expressed in the syntax. </p>

<p>For FORTH, that is atoms and words. FORTH is also good stuff, but pg isn&#39;t out there telling you to learn it. Maybe he should, we would get a lot of very reliable and hackable embedded systems in the bargain. </p>

<p>But the verdict is in: certain data types are just fundamental and we like having syntax that reflects this. Consider <code>$$$ foo bar baz bux $$$</code>, where <code>$$$</code> is our separator so that we don&#39;t get any hints. </p>

<p>Is that a list, such that adding some <code>qux</code> will give <code>$$$ qux foo bar baz bux $$$</code> ? Is it a vector, such that adding <code>qux</code> gives <code>$$$ foo bar baz bux qux $$$</code>?<br>
Perhaps a map, where adding qux wouldn&#39;t make sense, but adding <code>$$$ qux quux $$$</code> would give <code>$$$ foo bar, baz bux, qux quux $$$</code>? Or is it a set, where adding qux would give <code>$$$ foo qux baz bux bar $$$</code> (as an example order) but adding another foo would do nothing?</p>

<p>In J. Random Lisp, this is easy: <code>(vec foo bar)</code> <code>(map foo bar)</code> <code>(set foo bar)</code>. Or wait, does map make a map or map a function over some values? Maybe it&#39;s hash, or wait, does hash create a SHA? Arse, where&#39;s my documentation? I think set dynamically binds argument one to argument two&#8230;. </p>

<p>In Clojure, this is <code>(list foo)</code> <code>[vector foo]</code> <code>{:map foo}</code> <code>#{set foo}</code>. There are parenthetical forms of all of them, if necessary. </p>

<p>Note that these are not type categories. If you need that kind of thing, there&#39;s Haskell. These are <em>form</em> categories. There are only so many ways to use linear order to represent data, and Clojure&#39;s set of those is, as far as I can determine, exhaustive. Since linear order is all we have as long as we&#39;re making our programs out of strings, we now have the right amount of expressive power, for my taste. </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An Introduction to Ent]]></title>
    <link href="http://mnemnion.github.io/blog/2013/06/17/an-introduction-to-ent/"/>
    <updated>2013-06-17T10:11:00-07:00</updated>
    <id>http://mnemnion.github.io/blog/2013/06/17/an-introduction-to-ent</id>
    <content type="html"><![CDATA[<p>ent is a new approach to code creation. It is (will be) an editor and library that works on parse trees, rather than on files, and registers all changes as <a href="http://www.codecommit.com/blog/java/understanding-and-applying-operational-transformation">operational transformations</a>. It does so through the medium of familiar code files, but these may be thought of as an interface to the code, and as a product of it, similar to the executable binaries produced by a compiler. </p>

<h3 id="toc_0">Parse Aware Editing of Structured Files</h3>

<p>ent&#39;s major rationale is parse-awareness. It will, in general, not allow you to type invalid code, though this can always be overridden. It will parse your code as you create it, storing the resulting file as a series of operational transformations on the parse tree. As a language is more thoroughly defined within ent, this enables REPL-like instant feedback and sophisticated refactoring. </p>

<!-- more -->

<p>A simple example in json  will get us started. We are editing this file:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">[</span>  <span class="p">{</span><span class="nt">&quot;foo&quot;</span><span class="p">:</span><span class="s2">&quot;bar&quot;</span><span class="p">},</span>
</span><span class='line'>   <span class="err">_</span>
</span><span class='line'><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>Where <code>_</code> represents the cursor. We type <code>{</code>. </p>

<p>Because we are in an Array context, and the only rule that can match <code>{</code> is Object, we get:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">[</span>  <span class="p">{</span><span class="nt">&quot;foo&quot;</span><span class="p">:</span><span class="s2">&quot;bar&quot;</span><span class="p">},</span>
</span><span class='line'>   <span class="p">{</span><span class="err">_:**</span><span class="p">}</span>
</span><span class='line'><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>where <code>**</code> represents the target, which is the next place that ent expects us to move. </p>

<p>We now type <code>q</code>. Because we are in the Key context of an Object, this is not valid. But ent is friendly, so we get this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">[</span>  <span class="p">{</span><span class="nt">&quot;foo&quot;</span><span class="p">:</span><span class="s2">&quot;bar&quot;</span><span class="p">},</span>
</span><span class='line'>   <span class="p">{</span><span class="nt">&quot;q_&quot;</span><span class="p">:</span><span class="err">**</span><span class="p">}</span>
</span><span class='line'><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>Since json expects a string, the <code>&quot;&quot;</code> would actually be auto-inserted after the <code>{</code>. This example was somewhat contrived to show how ent can handle erroneous input through parse awareness.</p>

<p>Note, as an aside, that <a href="https://github.com/vmg/redcarpet">redcarpet&#39;s</a> json lexer identifies <code>**</code> as an error. This points to the advantage of parse aware editing, which can go far beyond syntax highlighting (as well as getting that task more correct than line-based regexes can).</p>

<p>We continue typing <code>ux</code> to give <code>qux</code>. Either <code>&quot;</code> or the right arrow key closes the string and gets us to our target:</p>

<p><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span><br>
<span class='line-number'>2</span><br>
<span class='line-number'>3</span><br>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">[</span>  <span class="p">{</span><span class="nt">&quot;foo&quot;</span><span class="p">:</span><span class="s2">&quot;bar&quot;</span><span class="p">},</span><br>
</span><span class='line'>   <span class="p">{</span><span class="nt">&quot;qux&quot;</span><span class="p">:</span><span class="err">_</span><span class="p">}</span> <span class="err">**</span><br>
</span><span class='line'><span class="p">]</span><br>
</span></code></pre></td></tr></table></div></figure><br>
Note that the target has moved also.</p>

<p>All of this special magic is enabled by the fact that ent cooperates with a parser to parse, validate, and transform the file as you type. ent will also have a &#39;permissive&#39; mode where the user may make arbitrary changes to the file; upon returning to opinionated mode, ent will reparse the edited regions and try and make sense of the input. </p>

<p>This flavor of convenience is well known to IDE users; ent generalizes this, but aims to do so in a way that has deep and far-reaching implications. </p>

<h3 id="toc_1">Continuous Comprehension</h3>

<p>As programmers, we want to stay close to our code as we work with it. The move from batch processing to interactive compile-run cycles was a boon, and the development of REPLs took us further, but we remain in a state where we interact with mutable flat files. </p>

<p>We would like to be in a state where we interact with immutable trees that embody not only the state of our program&#39;s encoding, but every state the program has ever been in. Graphic designers and CAD technicians have had this for decades; one may take the typical Adobe Photoshop file and run it backwards to the very first edit.</p>

<p>What is holding us back is that flat files are the ubiquitous interface between tools in the programming chain, and they are mutable by default (indeed, to a fault). ent aims to do the least possible to allow the move to immutable tree-based code structure while maintaining flat files in a sensible state and allowing other tools to act on and transform those files as input into the code structure. </p>

<h3 id="toc_2">Deep Waters</h3>

<p>This is not a trivial move. If we were free to design our own ball game, we could start with immutable data types, define transformations on them, and start snowballing. </p>

<p>ent isn&#39;t that kind of project. The entire towering edifice of computer software is built on transacting mutable text files; with few exceptions, everything running has a canonical form as a collection of such files, which is interacted upon by various tools to generate the running code. </p>

<p>ent wants to thoroughly change the method for generating that collection. Current best practice is revision control; ent extends that to dizzying heights, so dizzying, in fact, that it uses existing version control to keep matters from getting totally out of hand. </p>

<p>To say that modern code bases are mutable text file collections is imprecise. In most cases, they are exactly that, backed by a revision control package such as git. This manages those aspects of history and difference which the user has decided to record, and is an improvement. </p>

<p>ent stores and understands the code base as a series of operational transformations on a parse tree, and treats flat files as the canonical interface to that parse tree. let&#39;s break that down some before we continue.</p>

<p>To the user, a flat file is exactly what it was before. ent allows you to do whatever you want to it, in permissive mode, and the restrictions of opinionated mode are there to help; the user experience we&#39;re going for is one of free typing, with transformations, reformatting, and opinions offered in real time. Trying to type syntactic nonsense won&#39;t work in opinionated mode, which is very much the point.</p>

<p>To ent, proper, edits on the flat file are not seen. ent tracks the cursor position within the parse tree, and reparses the input periodically (specifically when the cursor crosses rule boundaries). It is the result of those parsing actions that ent tracks and stores, as operational transformations on the code base itself.</p>

<p>That is the minimum necessary to interact with a flat file through ent: a parser which meets certain interface criteria and produces a parse tree that contains every character in the original file. We can add more, but we cannot have less.</p>

<p>Even a small amount of structure can be useful. If we divide English into words, sentences, and paragraphs, ent will keep track of each one of these entities as they come into existence, and allow us to do many of the fanciful things ent makes possible, such as correcting a typo in a way that propagates to multiple copy-pasted versions of a sentence. </p>

<p>Importantly, even a poor or ambiguous grammar will work, as long as the parser&#39;s output is deterministic. The guarantee that ent will make you is that any tree it makes, if walked from left to right, will give you your string back. </p>

<h3 id="toc_3">Branch and Merge</h3>

<p>Because ent uses operational transformation, it provides great flexibility and control in branching and merging. OT is used in products like Google Docs so that, if network problems cause two user edit streams to diverge, the edits can be merged automatically into a single canonical document as soon as connectivity is restored. </p>

<p>ent approaches OT differently. Where Docs etc. are concerned with synchronizing multiple versions of a single canonical file in near-real time, ent uses OT to flexibly handle multiple branches and merges of a single code base, which may be replicated elsewhere with optional differences. </p>

<p>It is the same underlying algorithm, and it leads to a substantially different approach to branching and merging than that embodied in programs like git. Ultimately, ent enables time travel; you may return to any point in the history of your project, make revisions and changes, and propagate them, with control, back to the front of the project.</p>

<p>Let&#39;s contrast this with git. In git, to make a branch, you tell git you want to make a branch. git takes a snapshot and starts tracking changes under a different name. If you revert to the original branch, it goes back to the snapshot and tracks a different set of changes. When you merge, if all goes well, all the changes from both branches are reflected in the new file structure.</p>

<p>With ent, the user does not have to decide to branch. They may simply rewind to the point where an alternate path is helpful, create it, and merge. If all goes well, you have a new reality, in which the old edits happened in the past. </p>

<p>That&#39;s why we still use git within ent, for the record; when you start doing time travel, you start to wonder, sometimes, what reality used to look like. git, enslaved to ent, will serenely keep track of all this. </p>

<h3 id="toc_4">Time travel</h3>

<p>Here&#39;s some unavoidable terminology: in ent world, there are two universes. In Universe A, time is entropic, irreversible, and can only be queried as to prior state (and only through the mechanism of recording that prior state). In Universe B, time is reversible and mutable, with a higher order that immutably tracks the paths of that mutation and can unwind the skein accordingly. </p>

<p>What? Say I have a file, and I rewind time to rename a function <code>foo()</code> to <code>bar()</code>. I have a path of git revisions that say that at such-and-such a time, my file structure contains certain data. Since I haven&#39;t done any time traveling (it&#39;s not for the faint of heart), Universe A (git land) is the same as Universe B (ent space). </p>

<p>So I rewind time, past several git boundaries, and merge. It works. Now, if I go back in time in Universe B, my function is called <code>bar()</code>. If I go back in time in Universe A, my function is called <code>foo()</code>, until the moment that I went back in time, at which point it&#39;s called <code>bar()</code>. </p>

<p>Universe A is reality, as it happened. Universe B is reality as we wish it happened. They are a powerful team. </p>

<h3 id="toc_5">Branch and Merge, again</h3>

<p>The model is in principle no different when multiple authors work on one code base. ent tracks who made each change, in addition to what the change is, as part of the atomic transformation. </p>

<p>To the degree that one ent is aware of another, they may trade branches. Moreover, when changes are propagated up the time stream, they may be offered to such other ents as the propagating ent may be aware. </p>

<p>That&#39;s a lot of maybes. This is code we&#39;re talking about; handle with care. The current paradigm is pull-only for revisions; ent can provide notifications that changes are available, and hand those changes off, but pushing code willy-nilly is a bad habit to get into. </p>

<p>That said, there is often a clear division between fixing mistakes in code and extending / changing functionality. It is often the case that library updates will fix broken things and break working things, and careful use of ent can separate these concerns, by rewinding a local tree to the point where bad input was created and correcting the mistake. </p>

<p>In the real world, code sometimes depends on buggy behavior, and in this case, you simply rewind the edit and are stuck in the familiar position of having to either freeze the library or change your local codebase. Either way, merely providing the distinction between &#39;this is as things always should have been&#39; and &#39;this is how we want things to be now&#39; can prove powerful. </p>
]]></content>
  </entry>
  
</feed>
